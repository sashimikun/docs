---
title: "Model Providers Setup"
description: "Configure multiple AI model providers for optimal documentation generation with DeepWiki-Open"
---

DeepWiki-Open supports multiple AI model providers, each with unique strengths for different documentation needs. This guide covers setup, configuration, and optimization for all supported providers.

## Supported Providers

<CardGroup cols={2}>
<Card title="Google Gemini" icon="google" href="#google-gemini">
  Fast, reliable, generous free tier
</Card>

<Card title="OpenAI GPT" icon="openai" href="#openai">
  High-quality, detailed documentation
</Card>

<Card title="OpenRouter" icon="route" href="#openrouter">
  Access to 100+ models through one API
</Card>

<Card title="Azure OpenAI" icon="microsoft" href="#azure-openai">
  Enterprise-grade with enhanced security
</Card>

<Card title="AWS Bedrock" icon="aws" href="#aws-bedrock">
  AWS-hosted models with enterprise features
</Card>

<Card title="Ollama" icon="server" href="#ollama">
  Local, private, cost-free AI models
</Card>
</CardGroup>

## Google Gemini

Google's Gemini models offer excellent performance with generous free tiers, making them ideal for getting started.

### Setup

<Steps>
<Step title="Get API Key">
1. Visit [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Sign in with your Google account
3. Click "Create API Key"
4. Copy the generated key (starts with `AIza`)

<Check>
API key generated and copied
</Check>
</Step>

<Step title="Configure Environment">
Add to your `.env` file:

```env
GOOGLE_API_KEY=AIzaSyC...your_actual_key_here
```

<Warning>
Never commit API keys to version control. Add `.env` to your `.gitignore`.
</Warning>
</Step>

<Step title="Verify Setup">
Test the configuration by starting DeepWiki:

```bash
python -m api.main
# Should show: "Google API key configured successfully"
```
</Step>
</Steps>

### Available Models

<Tabs>
<Tab title="gemini-2.0-flash (Recommended)">
**Best for:** Most documentation tasks

- **Speed:** Very fast (1-3 seconds per request)
- **Quality:** Excellent for code analysis
- **Context:** 1M+ tokens input, 8K output
- **Cost:** Free tier: 15 RPM, 1M TPM

**Ideal for:**
- General repository documentation
- Quick prototyping and testing
- Regular development workflows
- Small to medium repositories
</Tab>

<Tab title="gemini-1.5-flash">
**Best for:** Stable, proven performance

- **Speed:** Fast (2-4 seconds per request)
- **Quality:** Very good, well-tested
- **Context:** 1M+ tokens input, 8K output
- **Cost:** Free tier: 15 RPM, 1M TPM

**Ideal for:**
- Production environments requiring stability
- Projects where consistency is critical
- Long-term documentation maintenance
</Tab>

<Tab title="gemini-1.0-pro">
**Best for:** Detailed analysis

- **Speed:** Moderate (3-6 seconds per request)
- **Quality:** High detail and accuracy
- **Context:** 32K tokens input/output
- **Cost:** Free tier: 60 RPM

**Ideal for:**
- Complex architectural analysis
- Detailed technical documentation
- Academic or research projects
</Tab>
</Tabs>

### Optimization Tips

<AccordionGroup>
<Accordion title="Rate Limit Management">
**Free tier limits:**
- 15 requests per minute (Flash models)
- 60 requests per minute (Pro models)
- 32,000 tokens per minute

**Best practices:**
```javascript
{
  "rate_limiting": {
    "requests_per_minute": 12,  // Stay below limit
    "retry_delay": 5,           // Wait 5s on rate limit
    "batch_processing": true    // Process files in batches
  }
}
```
</Accordion>

<Accordion title="Context Window Optimization">
Gemini models have large context windows. Optimize usage:

- **Large repositories**: Use full context for better understanding
- **Complex files**: Include more surrounding context
- **API documentation**: Include related endpoints together

```javascript
{
  "context_settings": {
    "max_file_size": 100000,     // 100KB per file
    "include_dependencies": true,  // Include related files
    "context_overlap": 0.1        // 10% overlap between chunks
  }
}
```
</Accordion>
</AccordionGroup>

## OpenAI

OpenAI's GPT models provide exceptional quality documentation with advanced reasoning capabilities.

### Setup

<Steps>
<Step title="Create Account & Get Credits">
1. Sign up at [OpenAI Platform](https://platform.openai.com/)
2. Add payment method (required for API access)
3. Purchase credits or set up billing
4. Navigate to [API Keys](https://platform.openai.com/api-keys)

<Warning>
OpenAI requires a paid account. Free ChatGPT accounts cannot access the API.
</Warning>
</Step>

<Step title="Generate API Key">
1. Click "Create new secret key"
2. Add a name (e.g., "DeepWiki-Development")
3. Copy the key (starts with `sk-`)
4. Store securely (you won't see it again)

<Check>
API key generated and stored securely
</Check>
</Step>

<Step title="Configure Environment">
```env
OPENAI_API_KEY=sk-proj-...your_actual_key_here
# Optional: Custom endpoint for compatible services
OPENAI_BASE_URL=https://api.openai.com/v1
```
</Step>
</Steps>

### Available Models

<Tabs>
<Tab title="gpt-5 (Latest - Default)">
**Best for:** State-of-the-art documentation generation with advanced reasoning

- **Speed:** Fast to moderate (3-8 seconds per request)
- **Quality:** Next-generation AI capabilities with superior understanding
- **Context:** 256K tokens input/output (estimated)
- **Temperature:** 1.0 (default for creative yet accurate responses)
- **Availability:** Rolling out to API users (check availability in your region)

**Ideal for:**
- Cutting-edge documentation projects
- Complex architectural documentation
- Multi-language codebases
- Advanced technical analysis
- Projects requiring latest AI capabilities

<Note>
GPT-5 is now the default model in DeepWiki as of commit 05693d5. Ensure your OpenAI account has access to GPT-5 API.
</Note>
</Tab>

<Tab title="gpt-4o (Previous Default)">
**Best for:** High-quality, comprehensive documentation

- **Speed:** Moderate (5-10 seconds per request)
- **Quality:** Exceptional writing and analysis
- **Context:** 128K tokens input/output
- **Cost:** $15/1M input tokens, $60/1M output tokens
- **Temperature:** 0.7 (default)
- **Top-p:** 0.8 (default)

**Ideal for:**
- Production documentation
- Complex enterprise applications
- Publication-quality content
- Detailed architectural analysis
- Fallback when GPT-5 is unavailable
</Tab>

<Tab title="gpt-4.1">
**Best for:** Enhanced reasoning and analysis

- **Speed:** Moderate to slow (8-15 seconds)
- **Quality:** Superior technical analysis
- **Context:** 128K tokens
- **Cost:** Premium pricing

**Ideal for:**
- Complex system analysis
- Advanced architectural documentation
- Research and academic projects
</Tab>

<Tab title="o1 Series (Reasoning Models)">
**Best for:** Complex problem solving and analysis

- **o1-preview:** Advanced reasoning, slower but thorough
- **o1-mini:** Faster reasoning for simpler tasks
- **Cost:** Higher than standard GPT-4

**Ideal for:**
- Complex debugging documentation
- System optimization analysis  
- Security assessment documentation
</Tab>

<Tab title="o4-mini (Cost-Effective)">
**Best for:** Budget-conscious high-quality documentation

- **Speed:** Fast (3-6 seconds per request)
- **Quality:** Very good for most tasks
- **Context:** 128K tokens
- **Cost:** Lower than GPT-4o

**Ideal for:**
- Regular documentation updates
- Smaller projects with quality requirements
- Development and testing workflows
</Tab>
</Tabs>

### Cost Optimization

<AccordionGroup>
<Accordion title="Token Usage Management">
Monitor and optimize token consumption:

```javascript
{
  "token_optimization": {
    "max_input_tokens": 100000,    // Limit input size
    "target_output_tokens": 4000,  // Reasonable output length
    "preprocessing": true,         // Clean input before sending
    "compression": "smart"         // Remove redundant content
  }
}
```

**Cost calculation example:**
- Large repository: ~200K input tokens, 8K output tokens
- GPT-5 cost: Pricing to be announced (expected similar or slightly higher than GPT-4o)
- GPT-4o cost: $3.00 input + $0.48 output = $3.48 per generation
- Monthly usage (10 repos): ~$35-50/month (estimated)
</Accordion>

<Accordion title="Model Selection Strategy">
**Match model to task complexity:**

1. **Simple projects:** Use o4-mini for cost savings
2. **Standard projects:** Use gpt-5 for latest capabilities or gpt-4o for proven reliability
3. **Complex analysis:** Use gpt-5 for advanced reasoning or o1 series for deep insights
4. **Budget constraints:** Start with o4-mini, upgrade if needed
5. **Cutting-edge needs:** Use gpt-5 for state-of-the-art performance

```javascript
{
  "auto_model_selection": {
    "repository_size": {
      "small": "o4-mini",      // < 100 files
      "medium": "gpt-5",       // 100-1000 files (if available, else gpt-4o)
      "large": "gpt-5"         // 1000+ files (if available, else gpt-4o)
    },
    "complexity_factors": [
      "multiple_languages",
      "microservice_architecture", 
      "complex_algorithms"
    ]
  }
}
```
</Accordion>
</AccordionGroup>

## OpenRouter

OpenRouter provides access to 100+ AI models through a single API, perfect for comparison and specialized needs.

### Setup

<Steps>
<Step title="Create Account">
1. Sign up at [OpenRouter](https://openrouter.ai/)
2. Verify your email address
3. Add payment method for paid models
4. Navigate to the Keys section

<Info>
Some models are free, others require credits. Check individual model pricing.
</Info>
</Step>

<Step title="Generate API Key">
1. Click "Create Key"
2. Name your key (e.g., "DeepWiki-Prod")
3. Copy the key (starts with `sk-or-`)
4. Optionally set spending limits

<Check>
OpenRouter API key generated with spending limits configured
</Check>
</Step>

<Step title="Configure Environment">
```env
OPENROUTER_API_KEY=sk-or-...your_actual_key_here
```
</Step>
</Steps>

### Popular Models

<Tabs>
<Tab title="Anthropic Claude">
**Models:** `anthropic/claude-3.5-sonnet`, `anthropic/claude-3-haiku`

**Best for:**
- Excellent code analysis and explanation
- Clear, structured documentation
- Complex reasoning tasks
- Safe, helpful responses

**Pricing:** $3/1M input tokens, $15/1M output tokens (3.5 Sonnet)

**Use cases:**
- API documentation generation
- Code architecture explanation
- Security-focused analysis
</Tab>

<Tab title="Google Models">
**Models:** `google/gemini-pro`, `google/gemini-pro-vision`

**Best for:**
- Multimodal analysis (code + diagrams)
- Fast processing
- Good balance of quality and speed

**Pricing:** Often lower than direct Google API
**Use cases:**
- Visual diagram analysis
- Multi-language projects
- Quick documentation updates
</Tab>

<Tab title="Open Source Models">
**Models:** `meta-llama/llama-3-70b`, `mistralai/mixtral-8x7b`

**Best for:**
- Cost-effective documentation
- Privacy-conscious projects
- Experimentation and development

**Pricing:** Usually $0.50-$2.00 per 1M tokens
**Use cases:**
- Large-scale documentation projects
- Internal/proprietary code analysis
- Development and testing
</Tab>

<Tab title="Specialized Models">
**Models:** `deepseek/deepseek-coder`, `phind/phind-codellama`

**Best for:**
- Code-specific analysis
- Programming language expertise
- Technical documentation

**Use cases:**
- Algorithm explanation
- Code optimization documentation
- Programming tutorial generation
</Tab>
</Tabs>

### Model Comparison Strategy

<Steps>
<Step title="Baseline Generation">
Start with a reliable, fast model:

```javascript
{
  "baseline_model": "anthropic/claude-3.5-sonnet",
  "test_repository": "https://github.com/small/test-repo"
}
```
</Step>

<Step title="A/B Testing">
Compare models for your specific use case:

```javascript
{
  "comparison_models": [
    "openai/gpt-4o",
    "google/gemini-pro", 
    "meta-llama/llama-3-70b"
  ],
  "evaluation_criteria": [
    "accuracy",
    "completeness", 
    "code_understanding",
    "diagram_quality",
    "cost_per_generation"
  ]
}
```
</Step>

<Step title="Optimization">
Select the best model based on results:

```javascript
{
  "selected_model": "anthropic/claude-3.5-sonnet",
  "reason": "Best code analysis with reasonable cost",
  "fallback_model": "google/gemini-pro",
  "reason_fallback": "Faster generation when speed needed"
}
```
</Step>
</Steps>

## Azure OpenAI

Enterprise-grade OpenAI models with enhanced security, compliance, and control.

### Setup

<Steps>
<Step title="Create Azure OpenAI Resource">
1. Sign in to [Azure Portal](https://portal.azure.com/)
2. Create new Azure OpenAI resource
3. Choose region (check model availability)
4. Configure pricing tier and network settings
5. Wait for deployment completion

<Info>
Azure OpenAI may require approval for access. Check the application status.
</Info>
</Step>

<Step title="Deploy Models">
1. Go to Azure OpenAI Studio
2. Navigate to Deployments
3. Deploy required models (GPT-4, GPT-3.5-turbo, etc.)
4. Note deployment names and endpoints

<Check>
Models deployed and endpoints configured
</Check>
</Step>

<Step title="Get Configuration Details">
Collect the required information:
- **Endpoint:** `https://your-resource.openai.azure.com`
- **API Key:** From resource keys section
- **API Version:** e.g., `2024-02-15-preview`

```env
AZURE_OPENAI_API_KEY=abc123...your_actual_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_VERSION=2024-02-15-preview
```
</Step>
</Steps>

### Enterprise Features

<AccordionGroup>
<Accordion title="Data Privacy & Compliance">
**Key benefits:**
- Data processed within your Azure tenant
- No data used for model training
- GDPR, SOC 2, HIPAA compliance available
- Private networking with VNet integration

**Configuration:**
```javascript
{
  "privacy_settings": {
    "data_residency": "eu-west",      // Keep data in specific region
    "logging": "minimal",             // Reduce data logging
    "retention": "30_days",           // Automatic data deletion
    "private_endpoint": true          // Use private networking
  }
}
```
</Accordion>

<Accordion title="Content Filtering">
**Built-in safety features:**
- Automatic content filtering for harmful content
- Customizable filter levels
- Compliance with organizational policies

**Configuration:**
```javascript
{
  "content_filter": {
    "severity_level": "medium",       // low, medium, high
    "categories": ["hate", "violence", "self_harm", "sexual"],
    "custom_blocklists": ["internal_terms"],
    "action_on_filter": "block"       // block, warn, log
  }
}
```
</Accordion>

<Accordion title="Scale & Performance">
**Enterprise-grade performance:**
- Dedicated capacity options
- Predictable performance
- Custom rate limits
- Multi-region deployment

**Configuration:**
```javascript
{
  "performance_settings": {
    "capacity_type": "provisioned",   // provisioned vs pay-per-token
    "throughput_units": 100,          // Dedicated throughput
    "auto_scaling": true,             // Scale with demand
    "load_balancing": "round_robin"   // Distribute across regions
  }
}
```
</Accordion>
</AccordionGroup>

## AWS Bedrock

AWS-hosted AI models with enterprise features and AWS service integration.

### Setup

<Steps>
<Step title="AWS Account Setup">
1. Ensure you have an AWS account
2. Enable AWS Bedrock in your region
3. Request access to required models (may require approval)
4. Create IAM user with Bedrock permissions

<Warning>
Bedrock is not available in all AWS regions. Check regional availability.
</Warning>
</Step>

<Step title="Configure IAM Permissions">
Create IAM policy for Bedrock access:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream"
      ],
      "Resource": "*"
    }
  ]
}
```
</Step>

<Step title="Configure Environment">
```env
AWS_ACCESS_KEY_ID=AKIA...your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_access_key_here
AWS_REGION=us-east-1
```

<Check>
AWS credentials configured and Bedrock access verified
</Check>
</Step>
</Steps>

### Available Models

<Tabs>
<Tab title="Anthropic Claude">
**Models:**
- `anthropic.claude-3-sonnet-20240229-v1:0`
- `anthropic.claude-3-haiku-20240307-v1:0`
- `anthropic.claude-3-opus-20240229-v1:0`

**Best for:** Code analysis, documentation, safety-conscious generation
**Pricing:** $3-15 per 1M tokens depending on model
</Tab>

<Tab title="Amazon Titan">
**Models:**
- `amazon.titan-text-express-v1`
- `amazon.titan-text-lite-v1`

**Best for:** Cost-effective text generation
**Pricing:** $0.50-2.00 per 1M tokens
</Tab>

<Tab title="AI21 Labs">
**Models:**
- `ai21.j2-ultra-v1`
- `ai21.j2-mid-v1`

**Best for:** Long-form documentation, detailed analysis
**Pricing:** Varies by model
</Tab>
</Tabs>

## Ollama (Local Models)

Run AI models locally for complete privacy, cost control, and offline capability.

### Setup

<Steps>
<Step title="Install Ollama">
<Tabs>
<Tab title="macOS">
```bash
# Using Homebrew
brew install ollama

# Or download installer from https://ollama.ai
curl -fsSL https://ollama.ai/install.sh | sh
```
</Tab>

<Tab title="Linux">
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start as system service
sudo systemctl enable ollama
sudo systemctl start ollama
```
</Tab>

<Tab title="Windows">
Download and install from [Ollama website](https://ollama.ai/download/windows)

Or use Windows Subsystem for Linux (WSL) with Linux instructions.
</Tab>

<Tab title="Docker">
```bash
# Run Ollama in Docker
docker run -d \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama

# Pull and run a model
docker exec -it ollama ollama run llama3:8b
```
</Tab>
</Tabs>
</Step>

<Step title="Pull Models">
Download models you want to use:

```bash
# Recommended models for documentation
ollama pull qwen3:8b          # Excellent for code
ollama pull llama3:8b         # Good general model
ollama pull qwen3:1.7b        # Lightweight option

# Verify installation
ollama list
```

<Check>
Models downloaded and verified
</Check>
</Step>

<Step title="Configure DeepWiki">
```env
OLLAMA_HOST=http://localhost:11434
```

For remote Ollama servers:
```env
OLLAMA_HOST=http://ollama-server.internal:11434
```
</Step>
</Steps>

### Model Selection

<Tabs>
<Tab title="Code-Focused Models">
**qwen3:8b (Recommended)**
- **Size:** 4.8GB download
- **RAM:** 8GB required
- **Strengths:** Excellent code understanding, multilingual
- **Best for:** Most documentation tasks

**deepseek-coder:6.7b**
- **Size:** 3.8GB download  
- **RAM:** 6GB required
- **Strengths:** Specialized for code generation and analysis
- **Best for:** Technical documentation, API docs
</Tab>

<Tab title="General Purpose Models">
**llama3:8b**
- **Size:** 4.7GB download
- **RAM:** 8GB required
- **Strengths:** Well-balanced, good reasoning
- **Best for:** General documentation, explanations

**llama3:70b** (High-end)
- **Size:** 40GB download
- **RAM:** 64GB+ required
- **Strengths:** Excellent quality, very detailed
- **Best for:** High-quality documentation with powerful hardware
</Tab>

<Tab title="Lightweight Options">
**qwen3:1.7b**
- **Size:** 1.0GB download
- **RAM:** 2GB required
- **Strengths:** Fast, efficient, good for simple tasks
- **Best for:** Quick documentation, low-resource environments

**tinyllama:1.1b**
- **Size:** 637MB download
- **RAM:** 1GB required  
- **Strengths:** Very fast, minimal resources
- **Best for:** Testing, simple explanations
</Tab>
</Tabs>

### Performance Optimization

<AccordionGroup>
<Accordion title="Hardware Requirements">
**Minimum specs by model size:**
- **1B-3B models:** 4GB RAM, any modern CPU
- **7B-8B models:** 8GB RAM, modern CPU (preferably 8+ cores)
- **13B models:** 16GB RAM, high-performance CPU
- **70B+ models:** 64GB+ RAM, server-grade hardware

**GPU acceleration (optional):**
```bash
# Enable GPU support (NVIDIA)
ollama pull llama3:8b
CUDA_VISIBLE_DEVICES=0 ollama run llama3:8b

# Check GPU usage
nvidia-smi
```
</Accordion>

<Accordion title="Memory Management">
**Optimize memory usage:**

```bash
# Set memory limits
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_MAX_QUEUE=4

# Configure model parameters
export OLLAMA_NUM_PARALLEL=2
export OLLAMA_FLASH_ATTENTION=1
```

**Model configuration:**
```javascript
{
  "model_config": {
    "num_ctx": 4096,          // Context window size
    "num_predict": 2048,      // Max output tokens
    "temperature": 0.7,       // Randomness
    "top_p": 0.8,            // Nucleus sampling
    "repeat_penalty": 1.1     // Avoid repetition
  }
}
```
</Accordion>
</AccordionGroup>

## Multi-Provider Strategy

### Provider Selection Matrix

<Tabs>
<Tab title="By Project Type">
| Project Type | Primary | Fallback | Reason |
|--------------|---------|----------|---------|
| **Open Source** | Google Gemini | OpenRouter | Free tier, good quality |
| **Enterprise** | Azure OpenAI | OpenAI | Security, compliance |  
| **Startup** | OpenRouter | Google | Cost optimization |
| **Research** | OpenAI GPT-4o | Claude via OpenRouter | Highest quality |
| **Personal** | Ollama | Google | Privacy, no cost |
</Tab>

<Tab title="By Repository Size">
| Size | Primary | Reason |
|------|---------|---------|
| **Small (&lt;100 files)** | Google Gemini Flash | Fast, sufficient quality |
| **Medium (100-1000)** | OpenAI GPT-4o | Better architecture analysis |
| **Large (1000+)** | Claude 3.5 Sonnet | Excellent at large contexts |
| **Enterprise** | Azure OpenAI | Security and compliance |
</Tab>

<Tab title="By Use Case">
| Use Case | Best Provider | Model | Why |
|----------|---------------|--------|-----|
| **API Documentation** | OpenAI | GPT-4o | Structured output |
| **Architecture Docs** | Anthropic (via OpenRouter) | Claude 3.5 Sonnet | System thinking |
| **Code Comments** | Google | Gemini Flash | Speed + accuracy |
| **Security Docs** | Azure OpenAI | GPT-4o | Enterprise security |
| **Cost-Conscious** | Ollama | Qwen3:8b | No API costs |
</Tab>
</Tabs>

### Auto-Failover Configuration

```javascript
{
  "provider_strategy": {
    "primary": {
      "provider": "google",
      "model": "gemini-2.0-flash",
      "timeout": 30
    },
    "fallback_chain": [
      {
        "provider": "openrouter", 
        "model": "anthropic/claude-3.5-sonnet",
        "condition": "rate_limit_exceeded"
      },
      {
        "provider": "ollama",
        "model": "qwen3:8b", 
        "condition": "network_error"
      }
    ],
    "retry_logic": {
      "max_retries": 3,
      "backoff_factor": 2,
      "jitter": true
    }
  }
}
```

## Next Steps

<CardGroup cols={2}>
<Card title="Authorization Mode" icon="shield" href="/getting-started/authorization-mode">
  Set up access control for your DeepWiki deployment
</Card>

<Card title="Generate First Wiki" icon="rocket" href="/getting-started/first-wiki">
  Create your first repository documentation
</Card>

<Card title="Production Setup" icon="cloud" href="/guides/production-setup">
  Deploy with multiple providers for production use
</Card>

<Card title="API Integration" icon="code" href="/api/overview">
  Integrate provider selection into your workflows
</Card>
</CardGroup>